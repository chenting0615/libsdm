/** ------------------------------------------------------------------------- *
 * libpomdp
 * ========
 * File: 
 * Description: Represent a POMDP model using a flat representation and
 *              sparse matrices and vectors. This class can be constructed
 *              from a pomdpSpecSparseMTJ object after parsing a .pomdp file.
 *              Sparse matrices by matrix-toolkits-java, 
 *              every matrix will be SparseMatrix:
 *              
 * S =
 *  (3,1)        1
 *  (2,2)        2
 *  (3,2)        3
 *  (4,3)        4
 *  (1,4)        5
 * A =
 *   0     0     0     5
 *   0     2     0     0
 *   1     3     0     0
 *   0     0     4     0
 * Copyright (c) 2009, 2010, 2011 Diego Maniloff
 * Copyright (c) 2010, 2011 Mauricio Araya
 --------------------------------------------------------------------------- */

package libsdm.mdp;

// imports
import java.io.Serializable;
import java.util.Random;

import libsdm.common.DenseVector;
import libsdm.common.Utils;
import libsdm.common.ValueSelector;

public class SparseMdp implements Mdp, Serializable {

	/**
	 * Generated by Eclipse.
	 */
	private static final long serialVersionUID = -5511401938934887929L;

	// ------------------------------------------------------------------------
	// properties
	// ------------------------------------------------------------------------

	// number of states
	protected int nrSta;

	// private nrAct
	protected int nrAct;

	// transition model: a x s x s'
	protected TransitionMatrix T;

	// reward model: a x s'
	protected RewardMatrix R;

	// discount factor
	protected double gamma;

	// action names
	protected String actStr[];

	// state names
	protected String staStr[];

	protected int init;

	// ------------------------------------------------------------------------
	// methods
	// ------------------------------------------------------------------------

	// / constructor
	public SparseMdp(TransitionMatrix T, RewardMatrix R, int nrSta, int nrAct,
			double gamma, String staStr[], String actStr[], int initState) {

		this.nrSta = T.states;
		this.nrAct = T.actions;
		this.gamma = gamma;
		this.actStr = actStr;
		this.staStr = staStr;
		this.init = initState;
		this.T=  T;
		this.R = R;
	} // constructor

	public SparseMdp(SparseMdp mdp) {
		this.nrSta = mdp.nrSta;
		this.nrAct = mdp.nrAct;
		this.T=  mdp.T;
		this.R = mdp.R;
		this.gamma = mdp.gamma;
		this.staStr = mdp.staStr;
		this.actStr = mdp.actStr;
		this.init = mdp.init;
	}

	public int states() {
		return nrSta;
	}

	public int actions() {
		return nrAct;
	}

	public double gamma() {
		return gamma;
	}

	public String getActionString(int a) {
		if (actStr == null)
			return null;
		return actStr[a];
	}

	public String getStateString(int s) {
		if (staStr == null)
			return null;
		return staStr[s];
	}

	public int getRandomAction() {
		return (Utils.gen.nextInt(Integer.MAX_VALUE) % actions());
	}

	public void changeRewardFunction(RewardFunction R) {
		if (!(R instanceof RewardMatrix))
			Utils.error("SparseMDP needs a RewardMatrix function");
		this.R = (RewardMatrix) R;
	}

	public ValueFunction backup(ValueFunction old, boolean async, int i) {
		RealValueFunction oldVf;
		if (old == null)
			oldVf = new RealValueFunction(nrSta, nrAct);
		else
			oldVf = (RealValueFunction) old;
		RealValueFunction newVf;
		newVf = oldVf.copy();
		for (int s = 0; s < nrSta; s++) {
			DenseVector vec;
			ValueSelector bests = new ValueSelector(ValueSelector.MAX);
			for (int a = 0; a < nrAct; a++) {
				//System.out.println("a="+a+" s="+s);
				if (async)
					vec = newVf.getValues().copy();
				else
					vec = oldVf.getValues().copy();
				//System.out.println(gamma);
				vec.scale(gamma);
				vec.add(R.get(s, a, i));
				//System.out.println(vec);
				//System.out.println(getTransitionMatrix().get(s, a, i).norm(1));
				double newval = vec.dot(getTransitionMatrix().get(s, a, i));
				//System.out.println(newval);
				newVf.setQ(s, a, newval);
				bests.put(new Integer(a), newval);
			}
			newVf.setValue(s, bests.getBest(), ((Integer)bests.getRandom()).intValue());
		}
		return newVf;
	}

	public int initialState() {
		return init;
	}

	public void setGamma(double d) {
		gamma=d;
	}

	public boolean isRewardStationary() {
		return R.stationary();
	}

	public boolean isTransitionStationary() {
		return getTransitionMatrix().stationary();
	}

	public int sampleNextState(int state, int action, Random gen, int i) {
		return getTransitionMatrix().sampleNextState(state, action, gen, i);
	}

	public RewardFunction getRewardFunction() {
		return R;
	}

	public TransitionMatrix getTransitionMatrix() {
		return (TransitionMatrix) T;
	}

} 

